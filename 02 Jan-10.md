# 02 Jan 2018

## 1.4 Independence

__Def__: Two events $E$ and $F$ are said to be independent, if $P(E\cap F)=P(E)\cdot P(F)$; denoted as $E\perp F$. __This is different rom disjoint.__

Assume $P(F)>0$, then $E\perp F \Leftrightarrow P(E|F)=P(E)$; intuitively, knowing $F$ does not change the probability of $E$.

__Proof__: 
$$\begin{aligned}
E\perp F & \Leftrightarrow P(E\cap F) = P(E)\cdot P(F) \\
         & \Leftrightarrow \frac{P(E\cap F)}{P(F)}=P(E) \\
         & \Leftrightarrow P(E|F)) = P(E)
\end{aligned}$$

More generally, a sequence of events $E_1, E_2, \ldots$ are called independent if for __any__ finite index set $I$,
$$
P(\bigcap_{i\in I}E_i)=\prod_{i\in I} P(E_i)
$$

## 1.5 Bayes's rule and law of total probability

__Theorem__: Let $F_1, F_2,\ldots$ be disjoint events, and $\bigcap_{i=1}^\infty F_i=\Omega$, we say $\{F_u\}_{i=1}^\infty$ forms a "partition" of the sample space $\Omega$

Then $P(E)=\sum_{i=1}^\infty P(E|F_i)\cdot P(F_i)$

__Proof__: exercie

Intuition: Decoupose the total probability into different cases.

!!!![image]!!! ask daniel
$$
P(E\cap F_2) = P(E|F+2)\cdot P(F_2)
$$

### Bayes' rule

$$
P(F_i | E) = \frac{P(E|F_i)\cdot P(F_i)}{\sum_{h=1}^\infty P(E|F_j)\cdot P(F_j)}
$$

_Bayes' rule_ tells us how to find conditional probability by switching the role of the event and the condition.

__Proof__:
$$\begin{aligned}
P(F_i | E)  & = \frac{P(F_i\cap E)}{P(E)}                               & \hspace{1em}\text{definition of condition probability} \\
            & = \frac{P(E|F_i)P(F_i)}{P(E)} \\ 
            & = \frac{P(E|F_i)P(F_i)}{\sum_{j=1}^\infty P(E|F_j)P(F_j)} & \text{law of total probability}
\end{aligned}$$

# 2 Random variables and distributions

## 2.1 Random variables

$(\Omega,\xi, P)$: Probability space.

__Def__: A random variable $X$ (or r.v.) is a mapping from $\Omega$ to $R$
$$\begin{aligned}
X : & \Omega\rightarrow R \\
    & \omega \rightarrow X(\omega)
\end{aligned}$$
A random variable transforms aribitrary "outcomes" into numbers.

!!!![image]!!! ask daniel

$X$ introduces a probability on $R$. For $A\subseteq R$, define
$$\begin{aligned}
P(X\in A) :&= P(\{X(\omega)\in A\}) \\
           &= P(\{\omega:X(\omega)\in A\}) \\
           &= P(X^{-1}(A))
\end{aligned}$$

From now on, we can often "forget" te original probability space and focus on the random variables and their distributions.

__Def__: let $X$ be a random variable. The __CDF__(cumulative distribution function) $F$ of $X$ is defined by 
$$
F(x) = P(X\leq x) = P(X\in(-\infty, x])) \\

X: \text{random variable}, x: \text{number}
$$

Properties of cdf:

1. $F$ is non-descreasing. $F(x_1)\leq F(x_2), x_1 < x_2$
2. * $\lim_{x\rightarrow -\infty}F(x) = 0$
   * $\lim_{x\rightarrow\infty}F(x)=1$
3. $F(x)$ is right continuous
   * $lim_{x\downarrow a}F(x) = F(a)$ : $x$ decreases to $a$(approaching from the right)
   * Hint: $\{x\leq a\}=\bigcap_{i=1}^\infty\{X\leq a_i\}$ for $a_i\downarrow a$
  
## 2.2 Discrete random variables and distributsion

A random variable $X$ is called __discrete__ if it only takes values in an __at most countable__ set $\{x_1, x_2, \ldots\}$ (finite or countable). 

The distribution of a discrete random variable is fully characterized by its __probability mass function__(p.m.f)

$$
    p(x):=P(X=x); x=x_1, x_2, \ldots
$$

Properties of pmf:

1. $p(x)\geq 0 \hspace{2em}\forall x$
2. $\sum_ip(x_i) = 1$

Q: what does the cdf of a discrete random variable look like?

### Examples of discrete distributions

#### 1.Bemoulli distribution

$$\begin{aligned}
p(1) &= P(X=1)=p \\
p(c) &= P(X=c) = 1-p \\
p(x) &= 0 otherwise
\end{aligned}$$
Denote $X\sim Ber(p)$

#### 2.Binomial dustruvytuib

$$\begin{aligned}
    p(k) = P(X=k) = \lgroup\stackrel{n}{k}\rgroup p^k(1-p)^{n-k}
\end{aligned}$$
* $X\sim Bin(n, p)$ to choose $k$ successes.
* Binomial distribution is the distribution of number of successes in $n$ independent trials; each having probability $p$ of suucess.
  
#### 3.Geometric distribution

$$
p(k) = P(X=k)=(1-p)^{k-1}p \\
(1-p)^{k-1}: \text{the first k-1 trials are all failures}, p: \text{success in kth trial}
$$
* $X\sim Geo(p)$
* $X$ is the number of trials needed to get the first success in $n$ independent trials with proability $p$ of success each
* $X$ has the memoryless property
$P(X>n+m|X>m) = P(x>n) \hspace{2em} n, m=0,1,\ldots$
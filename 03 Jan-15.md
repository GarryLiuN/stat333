# 2 Random variables and distributions (cont'd)

## 2.2 Discrete random variables and distributions(cont'd)

### 3.Geometric distribution

$$
p(k)=P(X=k)=(1-p)^{k-1}p
$$

Memoryless property:

$$
p(X>n+m|X>m)=P(X>n)a
$$

__Proof__:
$$
\begin{aligned}
P(X>k)  & = \sum_{j=k+1}^\infty P(X=j) \\
        & = \sum_{j=k+1}^\infty (i-p)^{j-1}p    \\
        & = (1-p)^kp \cdot \frac{1}{1-(1-p)}    \\
        & = (1-p)^k \\

P(X>n+m|x>m)    &= \frac{P(X>n+m), X>m}{P(X>m)} \\
                &= \frac{P(X>n+m)}{P(X>m)} = \frac{1-p)^{n+m}}{(1-p)^m} = (1-p)^n=P(X>n)
\end{aligned}
$$

__Intuition__: The failures in the past have no influence on how long we still need to wait to get the first success in the future

### 4. Poisson distribution

$$
p(k)=P(X=k)=\frac{\lambda^k e^{-\lambda}}{k!}, k=0,1,2,\ldots, \lambda > 0
$$

Other discrete distributions:

* negative binomial
* discrete uniform

## 2.3 Continuous random variables and distributions

__Definition__: A random variable $X$ is called __continuous__ if there exists a non-negative function $f$, such that for any interval $[a,b]$, $(a,b)$ or $[a,b)$:

$$
P(X\in[a,b]) = \int_a^b f(x)dx
$$

The function $f$ is called the _probability density function(pdf)_ of $X$

__Remark__: probability density function(pdf) is not probability. $P(X=x)=0$ if $X$ is continuous. The probability density function $f$ only gives probability when it is integrated.

If $X$ is continuous, then we can get cdf by:

$$
F(a)=P(X\in(-\infty, a])=\int^a_\infty f(x)dx
$$

hence, $F(x)$ is continuous, and differentiable "almost everywhere".

We can take $f(x)=F'(x)$ when the derivative exists, and $f(x)=$arbitrary number otherwise often to choose a value to make $f$ have some continuity.

Property of pdf:

1. $f(x)\leq 0$, $x\in R$
2. $\int_{-\infty}^\infty f(x)dx = 1$ 
3. For $A\subseteq R, P(X\in A)=\int_A f(x)dx$

### Example of continuous distribution

__Exponential distribution__
$$
f(x)=   \begin{cases}
        \begin{aligned}
            &\lambda e^{-\lambda x}  &, x\geq 0 \\
            &0                       &, x\leq 0
        \end{aligned}
        \end{cases} \\

X\sim Exp(x)
$$

Other continuous distributions:

* Normal distribution
* Uniform distribution

Exercises:

1. Find the cdf of $X\sim Exp(x)$
2. Show that the exponential distribution has the memoryless property:
   $$
   P(X>t+s|x>t)=P(X>s)
   $$

## 2.4 Joint distribution of r.v's

Let $X$ and $Y$ be two r.v's. defined on the same probability space $(\Omega, \xi, P)$

For each $\omega \in \Omega$, we have at the same time $X(\omega)$ and $Y(\omega)$. Then we can talk about the joint behavior of $X$ and $Y$

Two joint distribution of r.v's is characterized by joint cdf, joint pmf(discrete case) or joint pdf(continuous case).

* Joint cdf:
  * $F(x,y) = P(X<x, Y<y)$
* Joint pmf:
  * $f(x,y)=P(X=x, Y=y)$
* joint pdf $f(x,y)$ such that for $a<b, c<d$
  * $P(X,Y)\in(a,b]\times(c,d] = P(X\in(a,b], Y\in(c,d])=\int_a^b\int_c^d f(x,y)dy dx$
  * Equivalently:
    1. $F(x,y)=\int_{-\infty}^x\int_{-\infty}^y f(s,t)dtds \\$ $f(x,y)=\frac{\partial^2}{\partial x \partial y}F(x,y)$
    2. $P((X,Y)\in A) = \int\int_A f(x,y)dxdy$ for $A\subseteq R^2$

__Definition__: Two r.v's $X$ and $Y$ are called independent, if for all sets $A,B\subseteq R$, 
$$
P(X<A,Y<B)=P(X\in A)P(Y\in B)
$$
($\{X\in A\}$ and $\{Y\in B\}$ are independent events)

__Theorem__: Two r.v's $X$ and $Y$ are

1. independent, if and only if
2. $F(x,y)=F_x(x)F_y(y); x,y\in R$; where $F_x$: cdf of x; $F_y$: cdf of y
3. $f(x,y)=f_x(x)f_y(y); x,y\in R$; where $f$ is the joint pmf/pdf of $X$ and $Y$; $f_x$, $f_y$ are marginal pmf/pdf of $X$ and $Y$, respectively

__Proof__:

1.$\Rightarrow$ 2.

If $X \perp Y$, then by definition, 
$$
F(x,y)=P(X\in(-\infty, x],Y\in(-\infty, y])) = P(X\in(-\infty, x]))\cdot P(Y\in(-\infty,y])) = F_x(x)F_y(y)
$$

2.$\Rightarrow$ 3.

Assume $F(x,y)=F_x(x)\cdot F_y(y)$, 
$$
\begin{aligned}
f(x,y)=\frac{\partial^2}{\partial x \partial y}F(x,y)
    & = \frac{\partial^2}{\partial x \partial y} F_x(x)F_y(y) \\
    &= (\frac{\partial}{\partial x}F_x(x))(\frac{\partial}{\partial y}F_y(y)) \\
    &= f_x(x)f_y(y)
\end{aligned}
$$

3.$\Rightarrow$ 1.

Assume $f(x,y)=f_x(x)f_y(y)$; For $A,B\subseteq R$,
$$
\begin{aligned}
P(X\in A, Y\in B)   &= \int_{y\in B}\int_{x\in A} f(x,y) dxdy \\
                    &= \int_{y\in B}\int_{x\in A} f_x(x)f_y(y)dxdy \\
                    &= (\int_{x\in A} f_x(x)dx) (\int_{y\in B} f_y(y)dy) \\
                    &= P(X\in A)P(Y\in B)
\end{aligned}
$$

## 2.5 Expectation

__Definition__: For a r.v $X$, the expectation of $g(x)$ is defined as
$$
\exists(g(x))=  \begin{cases}
                \begin{aligned}
                    & \sum_{i=1}^\infty g(x_i)P(X=x_i) & \text{for discrete } X \\
                    & \int_{-\infty}^\infty g(x)f(x)dx & \text{for continuous} X
                \end{aligned}
                \end{cases}
$$
Let $X$,$Y$ be two r.v's; then the expectation of $g(X,Y)$ is defined in a similar way.

$$
\exists(g(x,y)) =   \begin{cases}
                        \begin{aligned}
                        &\sum\sum g(x_i,y_j)P(X=x_i, Y=y_j)\\
                        & \int\int g(x_i, y_j)f(x,y)dxdy
                        \end{aligned}
                    \end{cases}
$$
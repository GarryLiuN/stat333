# Note 14 - Mar 05

# Review

Period is a class property : $i\leftrightarrow  \Rightarrow d_i=d_j$. Irreducible $\rightarrow $ period of the MC

## Basic Limit Theorem

$\{X_n\}_{n=0,1,...}$ irreducible, aperiodic, positive recurrent DTMC; then an unique stationary distribution $\underline{\pi}=(\pi_0,\pi_1,...)$ exists. Moreover,

$$ 

(*)\underbrace{lim_{n-\rightarrow\infty} P_{ij}^{(n)}}_{\text{limiting distribution}\atop\text{(does not depend on the initial state i)} }

=lim_{n\rightarrow\infty}
\underbrace{\frac{\sum_{k=1}^n1\!\!\!\!\perp_{\{X_k=j\}}}{n}}_{\text{long-run fraction of time spent in j}}

=\underbrace{\frac{1}{\mathbb{E}(T_j|X_0=j)}}_{\text{$T_j=min\{n>0:X_n=j\}$}\atop\text{expected revisit time}}

=\pi_j\quad\quad, i,j\in S

$$

Periodic extension:

$$ 

\frac{\lim_{n\rightarrow\infty} P_{jj}^{(nd)}}{d}

= \lim_{n\rightarrow\infty}\frac{\sum_{k=1}^n1\!\!\!\!\perp_{\{X_k=j\}}}{n}

= \quad\frac{1}{\mathbb{E}(T_j|X_0=j)}=\pi_j

$$

Examples show:

* Irreducibility is related to the uniqueness of the stationary distribution;
* Aperiodicity is related to the existence of the limiting distribution.

# 4. Stochastic Processes (cont'd)

## 4.5 Limiting Distribution (cont'd)

### Example 4.5.3

$$ P_{o,j} = p_j, j=0,1,\cdots, p_0>0   \\
P_{i,i-1}=1, i\geq 1 $$

<p align="center">
    <img src="drawio_assets/example.4.5.3.svg">
</p>

Given $X_0=0$, $T_0=n+1$ if and only if $X_1=n$. his happens with prob $p_n$.

$$
\begin{aligned}
    \Rightarrow\mathbb{E}(T_0|X_0=0)
        &=\sum_{n=0}^\infty (n+1)p_n    \\
        &=1 + \sum_{n=0}^\infty np_n
\end{aligned}
$$

We can construct $p_n$ such that $\sum_{n=0}^\infty np_n=\infty$. (For example, $p_0=\frac{1}{2}, p_2=\frac{1}{4}, p_4=\frac{1}{4},\cdots$)

In this case, the chain is __null recurrent__. It is irreducible and aperiodic ($P_{00}=p_0>0$)

A stationary distribution does not exist. Reason: 
$$
p=
\begin{pmatrix}
    p_0 & p_1& p_2 & \cdots & p_i & \cdots \\
    1 && 0\\
    &1\\
    \cdots\\
    &&&&1
\end{pmatrix}   \\

\underline{\pi}\cdot P=\underline{\pi} \Rightarrow
$$
$$
\begin{aligned}
    &p_0\pi_0 + \pi_1 = \pi_0 \\
    &p_1\pi_0 + \pi_2 = \pi_1 \\
    &\quad\quad\vdots\\
    &p_{i-1}\pi_0 + \pi_i = \pi_{i-1} \\
    &p_i\pi_0 + \pi_{i+1} = \pi_I \\
\end{aligned}
$$

Add the first $i$ equations:
$$ (p_0+\cdots+p_{i-1})\pi_0 + (\cancel{\pi_1}+\cancel{\pi_2}+\cancel{\cdots}+\pi_i) = \pi_0 + \cancel{\cdots} + \cancel{\pi_{i-1}}\\

(p_0+\cdots+p_{i-1})\pi_0 + \pi_i = \pi_0\\

\Rightarrow \pi_i=(1-(p_o+\cdots+p_{i-1}))\pi_0 = \sum_{k=i}^\infty p_k\pi_0
$$

Try to normalize:

$$ \begin{aligned}
    1
        &= \sum_{i=1}^\infty \pi_i  \\
        &= \sum_{i=0}^\infty\sum_{k_i}^\infty p_k\pi_0 \\
        &= \sum_{k_i}^\infty \sum_{i=0}^\infty p_k\pi_0 \\
        &= \sum_{k_i}^\infty p_k \sum_{i=0}^\infty\pi_0 \\
        &= (\underbrace{\sum_{k_i}^\infty (k+1)p_k }_{\infty})\pi_0
\end{aligned} 
\\
\Rightarrow \pi_0=0\quad,\quad pi_i=0 \quad\forall i$$

This is not a distribution. Thus, a stationary distribution does not exist.

positive recurrence is related to the existence of the stationary distribution

### Example 4.5.4 : Electron

$$
P=\begin{pmatrix}
    1-\alpha    & \alpha \\
    \beta       & 1-\beta
\end{pmatrix}
\quad \alpha,\beta\in(0,1)$$

Irreducible, aperiodic, positive recurrence.

In order to find of $P^n$; we use the diagonalization technique.

$$
P=Q\Lambda Q^{-1} \quad\text{where $\Lambda$ is diagonal}\\\\

\Lambda = \begin{pmatrix}
    1   & 0 \\
    0   & 1-\alpha-\beta
\end{pmatrix}
\quad
Q=\begin{pmatrix}
    1   & \alpha  \\
    1   & 1-\beta
\end{pmatrix}
\quad
Q^{-1}=\frac{1}{\alpha+\beta}\begin{pmatrix}
    \beta   & \alpha\\
    1       & -1
\end{pmatrix}
$$
Then
$$ \begin{aligned}
P^n
    &= (Q\Lambda \cancel{Q}^{-1})(\cancel{Q}\Lambda \cancel{Q}^{-1})\cdot(\cancel{Q}\Lambda Q^{-1}) \\
    &= Q\Lambda^nQ^{n-1}    \\
    &= \begin{pmatrix}
            1   & \alpha    \\
            1   & -\beta
        \end{pmatrix}
        \begin{pmatrix}
            1   \\
            &   (1-\alpha-\beta)^n
        \end{pmatrix}
        \frac{1}{\alpha+\beta}
        \begin{pmatrix}
            \beta   & \alpha \\
            1       & -1
        \end{pmatrix}
        \\
    &=\frac{1}{\alpha+\beta}
        \begin{pmatrix}
            \beta+\alpha(1-\alpha-\beta)^n  & \alpha-\alpha(1-\alpha-\beta)^n   \\
            \beta-\beta(1-\alpha-\beta)^n  & \alpha+\beta(1-\alpha-\beta)^n
        \end{pmatrix}\\
    &\Rightarrow \lim_{n\rightarrow\infty}P^n=\frac{1}{\alpha+\beta}\begin{pmatrix}
           \beta & \alpha \\
           \beta & \alpha
        \end{pmatrix}
        =\begin{pmatrix}
            \frac{\beta}{\alpha+\beta} & \frac{\alpha}{\alpha+\beta}    \\  \\
            \frac{\beta}{\alpha+\beta} & \frac{\alpha}{\alpha+\beta}
        \end{pmatrix}
\end{aligned} $$

Note that $\lim_{n\rightarrow\infty} P^n$ has identical rows. This corresponds to the result that $\lim_{n\rightarrow\infty}P_{ij}^{(n)}$ does not depend on $i$

We saw that the stationary distribution $\underline{\pi}=(\frac{\beta}{\alpha+\beta},\frac{\alpha}{\alpha+\beta})$. So we verity that $\pi_j=\lim_{n\rightarrow\infty} P_{ij}^{(n)}$

Also, given $X_0=0$, $\mathbb{P}(T_0=1|X_0=0)=1-\alpha$ .

For $k=2,3,\cdots$

$$ \begin{aligned}
    \mathbb{P}(T_0=k|X_0=0) &= \mathbb{P}(X_k=0,X_{k-1}=1,\cdots, X_1=1|X_0=0)  \\
    &=\alpha(1-\beta)^{k-2}\beta\\
    &\Rightarrow \mathbb{E}(T_0|X_0=0)  \\
    &=1\cdot(1-\alpha)+\sum_{k=2}^\infty \alpha(1-\beta)^{k-2}\beta k   \\
    &=1-\alpha+\sum_{k=1}^\infty\underbrace{\alpha(1-\beta)^{k-2}\beta(k-1)}_{\mathbb{E}(Geo(\beta))}+\sum_{k=2}^\infty\alpha\underbrace{(1-\beta)^{k-2}\beta}_{\text{pmf of Geo($\beta$)}}\\
    &=1-\alpha+\alpha\sum_{k=1}^\infty(1-\beta)^{k-2}\beta(k-1)+\sum_{k=2}^\infty\alpha(1-\beta)^{k-2}\beta\\
    &=1\alpha+\alpha\cdot\frac{1}{\beta}+\alpha\cdot 1 \\
    &=1-\cancel{\alpha}+\frac{\alpha}{\beta}+\cancel{\alpha}\\
    &=\frac{\alpha+\beta}{\beta}
\end{aligned} $$

Hence we verify that $\mathbb{E}(T_0|X_0=0)=\frac{1}{\pi_0}$

## 4.6 Generating function and branching processes

### Definition 4.6.1

Let $\underline{p}=(p_0,p_1,\cdots)$ be a distribution on $\{0,1,2,\cdots\}$. Let $\xi$ be a r.v. following distribution $\underline{p}$. That is $\mathbb{P}(\xi=i)=p_i$. Then the generating function of $\xi$, or of $\underline{p}$, is defined by
$$ \begin{aligned}
    \psi(s)&=\mathbb{E}(s^\xi) \\
        &=\sum_{k=0}^\infty p_ks^k\quad\quad for 0\leq s\leq 1
\end{aligned} $$

### Properties of generating function

1. $\psi(0)=p_0,\quad\psi(1)=\sum_{k=0}^\infty p_k=1$
2. Generating function determines the distribution 
   $$ p_k=\frac{1}{k!}\frac{d^k\psi(s)}{ds^k}|_{s=0}$$
   Reason:
   $$ \psi(s)=p_0+p_1s^1+\cdots+p_{k-1}s^{k-1}+p_ks^k+p_{k+1}s^{k+1}+\cdots \\$$
   $$\frac{d^k\psi(s)}{ds^k}=k!p_k +(\cdots)s +(\cdots)s^2+\cdots$$
   $$ \frac{d^k\psi(s)}{ds^k}|_{s=0}=k!p_k $$
   In particular, $p_1\geq 0\Rightarrow\psi(s)$ is increasing. $p_2\geq 0\Rightarrow \psi(s)$ is climax
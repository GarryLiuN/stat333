# Note 17 - Mar 14

# Review

## Branching Processes

$$
\begin{aligned}
    &\mathbb{E}(Y) \leq 1 &\Rightarrow &\text{case } 2. \text{ Expectation probability }= 1  \\
    &\mathbb{E}(Y)>1 &\Rightarrow &\text{case } 1. \text{ Expectation probability is the smallest solution of } \psi(s)=s \text{ between $0$ and $1$}
\end{aligned}
$$

## Poisson Processes

### Definition

$$ 0 \leq S_1 \leq S_2 \leq \cdots $$
$$ N(t)=\sum_{n=1}^\infty 1\!\!\!\!\perp_{\{S_n\leq t\}} \text{ is the counting process of events } \{S_n\}_{n=1,2,...}$$

### Properties of a counting process

1. $N(t)\geq 0, t\geq 0$
2. $N(t)$ takes integer values
3. $N(t)$ is increasing.
   - $N(t_1)\leq N(t_2)$ if $t_1\leq t_2$
4. $N(t)$ is right-continuous
   - $N(t) = \lim_{s\downarrow t}N(s)$

Additional asumption: $N(0)=0, N(t)$ only has jumps with size $1$

# 5 Poisson Processes (cont'd)

### Recall : Properties of the Exponential Distributions

$X\sim Exp(\lambda)$

- Basic properties
  - pdf: $f(x)=\lambda e^{-\lambda x}\quad(x>0)$
  - cdf: $F(x)=1-e^{-\lambda x}$
  - $\mathbb{E}(x)=\frac{1}{\lambda}$
  - $Var(X)=\frac{1}{\lambda^2}$
- Memoryless property
  - $\mathbb{P}(X>s+t|x>s) = \mathbb{P}{x>t}$
- Min ofexponentials
  - $X_1,...,X_n$ independent, $X_i\sim Exp(\lambda_i)$, then
    1. $min(X_1, \cdots, X_n)\sim Exp(\lambda_1+\cdots+\lambda_n)\\$
        __Proof__: it suffices to prove the result for $n=2$
        $$ \text{Let } Z=min(X_1, X_2), \text{ then} $$
        $$ \begin{aligned}
            \mathbb{P}(Z>z)
                &= \mathbb{P}(X_1>z, X_2>z)\\
                &= \mathbb{P}(X_1>z)\cdot \mathbb{P}(X_2>z)  \\
                &= e^{-\lambda_1z}\cdot x^{-\lambda_2z} \\
                &= e^{-(\lambda_1+\lambda_2)z}
        \end{aligned} $$
        $$\Rightarrow\mathbb{P}(Z\leq z) = \underbrace{1-e^{-(\lambda_1+\lambda_2)z}}_{\text{cdf of $Exp(\lambda_1+\lambda_2)$}}\quad z>0 $$
        $$ Z\sim Exp(\lambda_1+\lambda_2) $$
    2. $\mathbb{P}(X_i=min(X_1,\cdots,X_2))=\frac{\lambda_i}{\lambda_1+\cdots+\lambda_n}\\$
        __Proof__: (again for $n=2$)
        $$
        \begin{aligned}
            &\mathbb{P}(X_1=min(X_1,X_2))               \\
            &= \mathbb{P}(X_1\leq X_2)                  \\
            &= \mathbb{E}(\mathbb{P}(X_1\leq X_2|X_1))  \\
            &= \mathbb{E}(e^{-\lambda_2X_1})            \\
            &= \int_0^\infty e^{-\lambda_2x}\lambda_1e^{-\lambda_1x} dx \\
            &= \lambda_1\int_0^\infty e^{-(\lambda_1+\lambda_2)x}dx \\
            &= \frac{\lambda_1}{\lambda_1+\lambda_2}
        \end{aligned}
        $$

## 5.3. Properties of Poisson Processes

### 5.3.1. Continuous-time Markov Property

$$
\begin{aligned}
    &\mathbb{P}(N(t_m)=j|N(t_{m-1})=i,N(t_{m-2})=i_{m-2},\cdots, N(t_1)=i_1) \\
    &\mathbb{P}(N(t_m)=j|N(t_{m-1}=i))
\end{aligned}
$$
for any $m$, $t_1<\cdots<t_m$, $i_1,i_2,\cdots, i_{m-2},i,j\in S$

#### Fact 5.3.1.1

The Poisson Process is the only renewal process having the Markov Property

__Reason__:

Since the exponential distribution is memoryless, the future arrival times will not depend on how long we have waited $\Rightarrow$ The future of the counting process only depends on its current value.

In fact,

$$ \begin{aligned}
    &\mathbb{P}(N(t+s)=j|N(s)+j)    \\
    \text{time homogeneity}&= \mathbb{P}(N(t)=j|N(0)=i)    \quad\text{only difference by which number we start ti ciybt}\\
    &= \mathbb{P}(N(t)=j-i|N(0)=0)
\end{aligned} $$

#### 5.3.1.1. Independent Increments

The Poisson Process has independent increments

$$ \begin{aligned}
    t_1<t_2<t_3<t_4
    \Rightarrow \underbrace{N(t_2) - N(t_1)}_{\text{increments}} \perp\!\!\!\perp \underbrace{N(t_4)-N(t_3)}_{\text{increments}}
\end{aligned} $$

__Reasons__:

Memoryless property of exponential distribution.

#### 5.3.1.2. Poisson Increments

The Poisson Process has Poisson increments

$$ N(t_2)-N(t_1)\sim Poi(\lambda(t_2-t_1)) $$

__Reason__:

Let the arrival times between $t_1$ and $t_2$ be $S_1,cdots, S_N$, where $N=N(t_2)-N(t_1)$. Then $W_1=S_1-t$. $W_2=S_2-S_1$, $\cdots$ are i.i.d r.v's with distribution $Exp(\lambda)$

$$
\begin{aligned}
    N=n\Leftrightarrow 
        &W_1+W_2+\cdots +W_n \leq t_2-t_1\\
        &W_1+W_2+\cdots +W_n+W_{n+1} > t_2-t_1
\end{aligned}
$$

#### Fact 5.3.1.2

If $W_1,\cdots,W_n$ are i.i.d. r.v's following $Exp(\lambda)$, then $W_1+\cdots+W_n\sim Erlang(n,\lambda)$(a special type of $Gamma$)
$$c.d.f: F(x)=1\sum_{k=1}^{n-1}\frac{1}{k!}e^{-\lambda x}(\lambda x)^k$$
Thus,
$$\begin{aligned}
    &\mathbb{P}(W_1+W_2+\cdots+W_n\leq t_2-t_1)\\
    &=1-\sum_{k=0}^{n-1}\frac{1}{k!}e^{-\lambda(t_2-t_1)}(\lambda(t_2-t_1))^k
\end{aligned}$$
$$\begin{aligned}
    &\mathbb{P}(W_1+W_2+\cdots+W_n+W_{n+1}\leq t_2-t_1)\\
    &=1-\sum_{k=0}^{n}\frac{1}{k!}e^{-\lambda(t_2-t_1)}(\lambda(t_2-t_1))^k
\end{aligned}$$
$$
\begin{aligned}
\mathbb{P}(N=n)
    &= \mathbb{P}(W_1+\cdots+W_n\leq t_2+t_1) - \mathbb{P}(W_1+\cdots+W_{n+1}\leq t_2-t_1)  \\
    &=\frac{1}{n!}e^{-\lambda(t_2-t_1)}(\lambda(t_2-t_1))^n
\end{aligned}
$$
In particular, $N(t)=N(t)-N(0)\sim Poi(\lambda t)$

$$ \mathbb{E}(N(1))=\lambda \quad \leftarrow \text{intensity: expected number of arrivals in one unit of time} $$

#### 5.3.1.3. Combining and Thining of Poisson Process

__Theorem__:
$$ \{N_1(t)\}\sim Poi(\lambda_1 t) $$
$$ \{N_2(t)\}\sim Poi(\lambda_2 t) $$
$\{N_1(t)\}$ and $\{N_2(t)\}$ are independent

Let $N(t)=N_1(t)+N_2(t)$, then $\{N(t)\}\sim Poi((\lambda_1+\lambda_2)t)$
